{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body Posture Classification using KNN and MediaPipe\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Extract pose landmarks from video frames using MediaPipe\n",
    "2. Train a KNN classifier to classify body postures (sitting, standing, laying)\n",
    "3. Evaluate the model performance\n",
    "4. Make predictions on new frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\u2705 All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pose Landmark Extraction\n",
    "\n",
    "First, let's create a class to extract pose landmarks from video frames using MediaPipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseLandmarkExtractor:\n",
    "    \"\"\"Extract pose landmarks from frames using MediaPipe\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize MediaPipe pose detection\"\"\"\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            static_image_mode=True,\n",
    "            model_complexity=2,\n",
    "            enable_segmentation=True,\n",
    "            min_detection_confidence=0.5\n",
    "        )\n",
    "        \n",
    "        # MediaPipe pose has 33 landmarks, each with x, y, z coordinates\n",
    "        self.num_landmarks = 33\n",
    "        self.features_per_landmark = 3  # x, y, z\n",
    "        self.total_features = self.num_landmarks * self.features_per_landmark  # 99 features\n",
    "    \n",
    "    def extract_landmarks_from_frame(self, frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract pose landmarks from a single frame\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame (BGR format)\n",
    "            \n",
    "        Returns:\n",
    "            Array of 99 features (33 landmarks \u00d7 3 coordinates)\n",
    "        \"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process with MediaPipe\n",
    "        results = self.pose.process(rgb_frame)\n",
    "        \n",
    "        # Initialize feature array\n",
    "        features = np.zeros(self.total_features)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Extract x, y, z coordinates for each landmark\n",
    "            for i, landmark in enumerate(landmarks):\n",
    "                features[i * 3] = landmark.x      # x coordinate\n",
    "                features[i * 3 + 1] = landmark.y  # y coordinate\n",
    "                features[i * 3 + 2] = landmark.z  # z coordinate\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_landmarks_from_frames(self, frames: List[np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract landmarks from multiple frames\n",
    "        \n",
    "        Args:\n",
    "            frames: List of frames\n",
    "            \n",
    "        Returns:\n",
    "            Array of shape (num_frames, 99) containing landmark features\n",
    "        \"\"\"\n",
    "        features_list = []\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            if i % 10 == 0:  # Progress update every 10 frames\n",
    "                print(f\"   Extracting landmarks from frame {i+1}/{len(frames)}...\")\n",
    "            features = self.extract_landmarks_from_frame(frame)\n",
    "            features_list.append(features)\n",
    "        \n",
    "        return np.array(features_list)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the pose detector\"\"\"\n",
    "        if self.pose:\n",
    "            self.pose.close()\n",
    "\n",
    "print(\"\u2705 PoseLandmarkExtractor class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Frame Extraction from Videos\n",
    "\n",
    "Function to extract frames from video files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_from_video(video_path: str, sample_rate: int = 60) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract frames from video file\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        sample_rate: Extract every nth frame\n",
    "        \n",
    "    Returns:\n",
    "        List of frames\n",
    "    \"\"\"\n",
    "    frames_array = []\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not video_capture.isOpened():\n",
    "        print(f\"Error: Could not open video file: {video_path}\")\n",
    "        return frames_array\n",
    "\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps if fps > 0 else 0\n",
    "\n",
    "    if fps <= 0:\n",
    "        print(f\"Warning: Unable to obtain framerate for video: {video_path}\")\n",
    "        video_capture.release()\n",
    "        return frames_array\n",
    "\n",
    "    print(f\"Processing: {os.path.basename(video_path)} (FPS: {fps:.1f}, Duration: {duration:.1f}s)\")\n",
    "\n",
    "    frame_index = 0\n",
    "    while True:\n",
    "        success, frame = video_capture.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        if frame_index % sample_rate == 0:\n",
    "            frames_array.append(frame)\n",
    "\n",
    "        frame_index += 1\n",
    "\n",
    "    video_capture.release()\n",
    "    print(f\"\u2705 Extracted {len(frames_array)} frames\")\n",
    "    return frames_array\n",
    "\n",
    "print(\"\u2705 Frame extraction function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation\n",
    "\n",
    "Let's prepare our dataset by extracting landmarks from videos in each posture class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data_dir: str, max_videos_per_class: int = 20) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepare dataset from video files in Data directory\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to Data directory\n",
    "        max_videos_per_class: Maximum number of videos to process per class\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (features, labels)\n",
    "    \"\"\"\n",
    "    print(\"\ud83d\udcca Preparing dataset from video files...\")\n",
    "    \n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Initialize landmark extractor\n",
    "    extractor = PoseLandmarkExtractor()\n",
    "    \n",
    "    try:\n",
    "        # Process each posture class\n",
    "        class_mapping = {\n",
    "            'Sitting': 0,\n",
    "            'Standing': 1, \n",
    "            'laying': 2\n",
    "        }\n",
    "        \n",
    "        for posture, class_idx in class_mapping.items():\n",
    "            print(f\"\\n\ud83c\udfaf Processing {posture} posture...\")\n",
    "            \n",
    "            posture_dir = Path(data_dir) / posture\n",
    "            if not posture_dir.exists():\n",
    "                print(f\"   \u26a0\ufe0f  Directory {posture} not found, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Get video files\n",
    "            video_files = list(posture_dir.glob(\"*.mp4\"))\n",
    "            print(f\"   \ud83d\udcf9 Found {len(video_files)} video files\")\n",
    "            \n",
    "            # Process videos (limit to avoid long processing)\n",
    "            for i, video_path in enumerate(video_files[:max_videos_per_class]):\n",
    "                print(f\"   Processing video {i+1}/{min(len(video_files), max_videos_per_class)}: {video_path.name}\")\n",
    "                \n",
    "                # Extract frames\n",
    "                frames = extract_frames_from_video(str(video_path), sample_rate=60)\n",
    "                \n",
    "                if frames:\n",
    "                    # Extract landmarks from frames\n",
    "                    frame_features = extractor.extract_landmarks_from_frames(frames)\n",
    "                    \n",
    "                    # Add features and labels\n",
    "                    features_list.extend(frame_features)\n",
    "                    labels_list.extend([class_idx] * len(frame_features))\n",
    "                    \n",
    "                    print(f\"     \u2705 Extracted {len(frame_features)} frames\")\n",
    "                else:\n",
    "                    print(f\"     \u274c No frames extracted from {video_path.name}\")\n",
    "    \n",
    "    finally:\n",
    "        extractor.close()\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    features = np.array(features_list)\n",
    "    labels = np.array(labels_list)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Dataset prepared:\")\n",
    "    print(f\"   Total samples: {len(features)}\")\n",
    "    print(f\"   Features per sample: {features.shape[1]}\")\n",
    "    \n",
    "    # Print class distribution\n",
    "    class_names = ['sitting', 'standing', 'laying']\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    for class_idx, count in zip(unique, counts):\n",
    "        print(f\"   {class_names[class_idx]}: {count} samples\")\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# Prepare dataset (limit to 10 videos per class for faster processing)\n",
    "features, labels = prepare_dataset(\"Data\", max_videos_per_class=10)\n",
    "\n",
    "print(f\"\\n\u2705 Dataset preparation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Visualization\n",
    "\n",
    "Let's visualize the class distribution and feature statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Class distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "class_names = ['sitting', 'standing', 'laying']\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "plt.bar(class_names, counts)\n",
    "plt.title('Class Distribution')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Feature statistics\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(features.flatten(), bins=50, alpha=0.7)\n",
    "plt.title('Feature Value Distribution')\n",
    "plt.xlabel('Feature Values')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Feature correlation heatmap (first 20 features)\n",
    "plt.subplot(1, 3, 3)\n",
    "correlation_matrix = np.corrcoef(features[:, :20].T)\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, \n",
    "            xticklabels=range(1, 21), yticklabels=range(1, 21))\n",
    "plt.title('Feature Correlation (First 20 Features)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2705 Data visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. KNN Classifier Training\n",
    "\n",
    "Now let's train our KNN classifier with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\u2705 Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for KNN\n",
    "print(\"\ud83d\udd0d Performing hyperparameter tuning...\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(\n",
    "    knn, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"\ud83d\udcca Best cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Get best model\n",
    "best_knn = grid_search.best_estimator_\n",
    "\n",
    "print(\"\u2705 Hyperparameter tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model and make predictions\n",
    "print(\"\ud83c\udfaf Training final KNN model...\")\n",
    "\n",
    "# Train on full training set\n",
    "best_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_knn.predict(X_test_scaled)\n",
    "y_pred_proba = best_knn.predict_proba(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\u2705 Training completed!\")\n",
    "print(f\"\ud83d\udcca Test accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Let's evaluate our model performance with detailed metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "print(\"\ud83d\udcca Model Evaluation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\ud83d\udccb Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\n\ud83c\udfaf Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross-validation score\n",
    "print(\"\\n\ud83d\udd04 Cross-Validation Score:\")\n",
    "cv_scores = cross_val_score(best_knn, X_train_scaled, y_train, cv=5)\n",
    "print(f\"   Mean CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "print(\"\u2705 Model evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis\n",
    "\n",
    "Let's analyze which pose landmarks are most important for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis using permutation importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "print(\"\ud83d\udd0d Analyzing feature importance...\")\n",
    "\n",
    "# Calculate permutation importance\n",
    "result = permutation_importance(best_knn, X_test_scaled, y_test, \n",
    "                               n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Get feature names (landmark coordinates)\n",
    "landmark_names = []\n",
    "for i in range(33):  # 33 landmarks\n",
    "    landmark_names.extend([f\"Landmark_{i}_x\", f\"Landmark_{i}_y\", f\"Landmark_{i}_z\"])\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Top 20 most important features\n",
    "top_indices = np.argsort(result.importances_mean)[-20:]\n",
    "top_features = [landmark_names[i] for i in top_indices]\n",
    "top_importances = result.importances_mean[top_indices]\n",
    "\n",
    "plt.barh(range(len(top_features)), top_importances)\n",
    "plt.yticks(range(len(top_features)), top_features)\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Top 20 Most Important Pose Landmarks')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2705 Feature importance analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Saving and Loading\n",
    "\n",
    "Let's save our trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model and scaler\n",
    "model_data = {\n",
    "    'knn': best_knn,\n",
    "    'scaler': scaler,\n",
    "    'class_names': class_names,\n",
    "    'feature_names': landmark_names\n",
    "}\n",
    "\n",
    "joblib.dump(model_data, 'body_posture_classifier.pkl')\n",
    "print(\"\u2705 Model saved to 'body_posture_classifier.pkl'\")\n",
    "\n",
    "# Test loading the model\n",
    "loaded_model_data = joblib.load('body_posture_classifier.pkl')\n",
    "loaded_knn = loaded_model_data['knn']\n",
    "loaded_scaler = loaded_model_data['scaler']\n",
    "loaded_class_names = loaded_model_data['class_names']\n",
    "\n",
    "print(\"\u2705 Model loaded successfully!\")\n",
    "print(f\"   Classes: {loaded_class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-time Prediction Function\n",
    "\n",
    "Create a function to predict posture from a single frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_posture_from_frame(frame: np.ndarray, model_data: dict) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Predict posture for a single frame\n",
    "    \n",
    "    Args:\n",
    "        frame: Input frame (BGR format)\n",
    "        model_data: Loaded model data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (predicted_class, confidence)\n",
    "    \"\"\"\n",
    "    # Extract landmarks\n",
    "    extractor = PoseLandmarkExtractor()\n",
    "    try:\n",
    "        features = extractor.extract_landmarks_from_frame(frame)\n",
    "        features_scaled = model_data['scaler'].transform(features.reshape(1, -1))\n",
    "        \n",
    "        # Predict\n",
    "        prediction = model_data['knn'].predict(features_scaled)[0]\n",
    "        confidence = model_data['knn'].predict_proba(features_scaled)[0].max()\n",
    "        \n",
    "        return model_data['class_names'][prediction], confidence\n",
    "    finally:\n",
    "        extractor.close()\n",
    "\n",
    "# Test prediction on a sample frame\n",
    "if len(X_test) > 0:\n",
    "    # Use the first test sample as example\n",
    "    sample_features = X_test[0]\n",
    "    sample_features_scaled = scaler.transform(sample_features.reshape(1, -1))\n",
    "    \n",
    "    prediction = best_knn.predict(sample_features_scaled)[0]\n",
    "    confidence = best_knn.predict_proba(sample_features_scaled)[0].max()\n",
    "    \n",
    "    print(f\"\\n\ud83e\uddea Sample Prediction Test:\")\n",
    "    print(f\"   True class: {class_names[y_test[0]]}\")\n",
    "    print(f\"   Predicted class: {class_names[prediction]}\")\n",
    "    print(f\"   Confidence: {confidence:.3f}\")\n",
    "\n",
    "print(\"\u2705 Prediction function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "Let's summarize our results and suggest improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83c\udf89 Body Posture Classification Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\ud83d\udcca Dataset size: {len(features)} samples\")\n",
    "print(f\"\ud83d\udd22 Features per sample: {features.shape[1]} (33 landmarks \u00d7 3 coordinates)\")\n",
    "print(f\"\ud83c\udfaf Best KNN parameters: {grid_search.best_params_}\")\n",
    "print(f\"\ud83d\udcc8 Test accuracy: {accuracy:.3f}\")\n",
    "print(f\"\ud83d\udd04 Cross-validation accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "print(f\"\ud83d\udcbe Model saved: body_posture_classifier.pkl\")\n",
    "\n",
    "print(\"\\n\ud83d\ude80 Next Steps:\")\n",
    "print(\"1. Collect more data for better class balance\")\n",
    "print(\"2. Try other algorithms (Random Forest, SVM, Neural Networks)\")\n",
    "print(\"3. Implement real-time video classification\")\n",
    "print(\"4. Add data augmentation techniques\")\n",
    "print(\"5. Fine-tune MediaPipe parameters\")\n",
    "\n",
    "print(\"\\n\u2705 Classification pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (posture-env-py38)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}